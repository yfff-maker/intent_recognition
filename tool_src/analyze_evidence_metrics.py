"""
è¯æ®è´¨é‡æŒ‡æ ‡åˆ†æ - ABCç­–ç•¥å¯¹æ¯”

åˆ†ææŒ‡æ ‡ï¼š
1. Early-evidence rateï¼ˆæ—©æœŸè¯æ®å¼•ç”¨ç‡ï¼‰ï¼šè¯æ®ä¸­æœ‰å¤šå°‘æ¥è‡ªçª—å£å·¦ä¾§
2. Average evidence distanceï¼ˆè¯æ®å¹³å‡è·ç¦»ï¼‰ï¼šè¯æ®äº‹ä»¶ç¦»å¼‚å¸¸ç‚¹çš„å¹³å‡è·ç¦»
3. çª—å£äº‹ä»¶æ•°
4. ä¸Šä¸‹æ–‡tokenæ•°

è¾“å‡º4å¼ å›¾ï¼š
- çª—å£äº‹ä»¶æ•° vs Early-evidence rate
- çª—å£äº‹ä»¶æ•° vs Average evidence distance
- ä¸Šä¸‹æ–‡tokenæ•° vs Early-evidence rate
- ä¸Šä¸‹æ–‡tokenæ•° vs Average evidence distance
"""

import os
import json
import re
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import rcParams
from typing import List, Dict, Tuple

# è®¾ç½®ä¸­æ–‡å­—ä½“
rcParams['font.sans-serif'] = ['Microsoft YaHei', 'SimHei', 'Arial Unicode MS']
rcParams['axes.unicode_minus'] = False

OUTPUT_DIR = "./output"

# çª—å£äº‹ä»¶æ•°ï¼ˆç†è®ºå€¼ï¼Œæ¥è‡ªconfig.pyï¼‰
WINDOW_SIZES = {
    'A': {'k_left': 2, 'k_right': 2, 'total': 5},
    'B': {'k_left': 20, 'k_right': 20, 'total': 41},
    'C': {'k_left': 200, 'k_right': 50, 'total': 251},
}


def parse_evidence_field(evidence_str: str) -> List[Dict]:
    """
    è§£æEvidenceå­—æ®µï¼ˆå¯èƒ½æ˜¯å­—ç¬¦ä¸²åŒ–çš„JSONï¼‰
    
    è¿”å›: [{"event_idx": "42", "why": "..."}, ...]
    """
    if pd.isna(evidence_str) or evidence_str == '[]':
        return []
    
    try:
        # å°è¯•ç›´æ¥è§£æJSON
        evidence = json.loads(evidence_str)
        if isinstance(evidence, list):
            return evidence
    except:
        pass
    
    # å°è¯•å­—ç¬¦ä¸²å½¢å¼
    try:
        # æ›¿æ¢å•å¼•å·ä¸ºåŒå¼•å·
        evidence_str = str(evidence_str).replace("'", '"')
        evidence = json.loads(evidence_str)
        if isinstance(evidence, list):
            return evidence
    except:
        pass
    
    return []


def extract_event_indices(evidence_list: List[Dict]) -> List[int]:
    """
    ä»evidenceåˆ—è¡¨ä¸­æå–äº‹ä»¶ç´¢å¼•
    
    event_idxå¯èƒ½æ˜¯ï¼š
    - "42"ï¼ˆå•ä¸ªç´¢å¼•ï¼‰
    - "42..45"ï¼ˆèŒƒå›´ï¼‰
    - "chunk_P1_2"ï¼ˆchunkå¼•ç”¨ï¼Œå¿½ç•¥ï¼‰
    """
    indices = []
    
    for item in evidence_list:
        idx_str = item.get('event_idx', '')
        
        if not idx_str or 'chunk' in idx_str.lower():
            continue  # è·³è¿‡chunkå¼•ç”¨
        
        # å¤„ç†èŒƒå›´ "42..45"
        if '..' in idx_str:
            try:
                parts = idx_str.split('..')
                start = int(parts[0])
                end = int(parts[1])
                indices.extend(range(start, end + 1))
            except:
                pass
        else:
            # å•ä¸ªç´¢å¼•
            try:
                indices.append(int(idx_str))
            except:
                pass
    
    return indices


def estimate_token_count(prompt_text: str) -> int:
    """
    ä¼°ç®—promptçš„tokenæ•°
    
    ç®€å•ä¼°ç®—ï¼š1 token â‰ˆ 4 charactersï¼ˆè‹±æ–‡+ä¸­æ–‡æ··åˆï¼‰
    """
    if pd.isna(prompt_text):
        return 0
    
    char_count = len(str(prompt_text))
    token_count = char_count // 4
    return token_count


def calculate_evidence_metrics(row, center_event_idx: int) -> Dict:
    """
    è®¡ç®—å•ä¸ªå¼‚å¸¸ç‚¹çš„è¯æ®æŒ‡æ ‡
    
    Args:
        row: DataFrameçš„ä¸€è¡Œ
        center_event_idx: å¼‚å¸¸ç‚¹å¯¹åº”çš„ä¸­å¿ƒäº‹ä»¶ç´¢å¼•
    
    Returns:
        {
            'early_evidence_rate': float,  # æ—©æœŸè¯æ®æ¯”ä¾‹
            'avg_evidence_distance': float,  # å¹³å‡è·ç¦»
            'total_evidence_count': int,  # æ€»è¯æ®æ•°
        }
    """
    # è§£æEvidenceå­—æ®µ
    evidence_list = parse_evidence_field(row['Evidence'])
    
    if not evidence_list:
        return {
            'early_evidence_rate': np.nan,
            'avg_evidence_distance': np.nan,
            'total_evidence_count': 0,
        }
    
    # æå–äº‹ä»¶ç´¢å¼•
    indices = extract_event_indices(evidence_list)
    
    if not indices:
        return {
            'early_evidence_rate': np.nan,
            'avg_evidence_distance': np.nan,
            'total_evidence_count': len(evidence_list),  # å¯èƒ½éƒ½æ˜¯chunkå¼•ç”¨
        }
    
    # è®¡ç®—æ—©æœŸè¯æ®ç‡ï¼ˆç´¢å¼• < center_event_idx çš„æ¯”ä¾‹ï¼‰
    early_count = sum(1 for idx in indices if idx < center_event_idx)
    early_rate = early_count / len(indices) if len(indices) > 0 else 0
    
    # è®¡ç®—å¹³å‡è·ç¦»ï¼ˆç»å¯¹å€¼ï¼‰
    distances = [abs(idx - center_event_idx) for idx in indices]
    avg_distance = np.mean(distances) if distances else 0
    
    return {
        'early_evidence_rate': early_rate,
        'avg_evidence_distance': avg_distance,
        'total_evidence_count': len(indices),
    }


def load_and_process_data():
    """åŠ è½½Banditç»“æœå¹¶å¤„ç†"""
    bandit_path = os.path.join(OUTPUT_DIR, "intent_inference_results_bandit.xlsx")
    
    if not os.path.exists(bandit_path):
        print(f"âŒ æœªæ‰¾åˆ°Banditç»“æœæ–‡ä»¶: {bandit_path}")
        return None
    
    df = pd.read_excel(bandit_path)
    print(f"âœ“ å·²åŠ è½½ {len(df)} æ¡è®°å½•")
    
    # ä¸ºæ¯è¡Œè®¡ç®—æŒ‡æ ‡
    print("\nå¤„ç†ä¸­...", end='')
    
    results = []
    
    for idx, row in df.iterrows():
        if idx % 50 == 0:
            print(".", end='', flush=True)
        
        strategy = row['Strategy']
        
        # ä¼°ç®—çª—å£å¤§å°å’Œtokenæ•°
        window_size = WINDOW_SIZES[strategy]['total']
        token_count = estimate_token_count(row['Prompt'])
        
        # å‡è®¾ä¸­å¿ƒäº‹ä»¶ç´¢å¼•ï¼ˆç®€åŒ–ï¼šæˆ‘ä»¬ä¸çŸ¥é“ç¡®åˆ‡çš„center_posï¼Œç”¨timestampä¼°ç®—ï¼‰
        # åœ¨å®é™…åœºæ™¯ä¸­ï¼Œå¯èƒ½éœ€è¦ä»å…¶ä»–åœ°æ–¹è·å–
        # è¿™é‡Œæˆ‘ä»¬ç”¨ä¸€ä¸ªè¿‘ä¼¼æ–¹æ³•ï¼šä»Evidenceä¸­æ¨æ–­
        evidence_list = parse_evidence_field(row['Evidence'])
        indices = extract_event_indices(evidence_list)
        
        # è¿‘ä¼¼ä¸­å¿ƒç‚¹ï¼šå‡è®¾æ˜¯è¯æ®ç´¢å¼•çš„ä¸­ä½æ•°é™„è¿‘
        if indices:
            center_event_idx = int(np.median(indices))
        else:
            center_event_idx = 0  # æ— æ³•ç¡®å®šï¼Œè®¾ä¸º0
        
        # è®¡ç®—è¯æ®æŒ‡æ ‡
        metrics = calculate_evidence_metrics(row, center_event_idx)
        
        results.append({
            'Participant': row['Participant'],
            'AnchorTimestamp': row['AnchorTimestamp'],
            'AnomalyType': row['AnomalyType'],
            'Strategy': strategy,
            'WindowSize': window_size,
            'TokenCount': token_count,
            'Confidence': row['Confidence'],
            'EarlyEvidenceRate': metrics['early_evidence_rate'],
            'AvgEvidenceDistance': metrics['avg_evidence_distance'],
            'TotalEvidenceCount': metrics['total_evidence_count'],
            'Intent': row['Intent'],
        })
    
    print(" å®Œæˆï¼")
    
    df_processed = pd.DataFrame(results)
    return df_processed


def aggregate_by_strategy(df):
    """æŒ‰ç­–ç•¥èšåˆç»Ÿè®¡"""
    print("\n" + "="*80)
    print("ğŸ“Š æŒ‰ç­–ç•¥èšåˆçš„æŒ‡æ ‡ç»Ÿè®¡")
    print("="*80)
    
    agg_results = []
    
    for strategy in ['A', 'B', 'C']:
        df_s = df[df['Strategy'] == strategy]
        
        stats = {
            'Strategy': strategy,
            'WindowSize_Mean': df_s['WindowSize'].mean(),
            'TokenCount_Mean': df_s['TokenCount'].mean(),
            'TokenCount_Std': df_s['TokenCount'].std(),
            'Confidence_Mean': df_s['Confidence'].mean(),
            'Confidence_Std': df_s['Confidence'].std(),
            'EarlyEvidenceRate_Mean': df_s['EarlyEvidenceRate'].mean(),
            'EarlyEvidenceRate_Std': df_s['EarlyEvidenceRate'].std(),
            'AvgEvidenceDistance_Mean': df_s['AvgEvidenceDistance'].mean(),
            'AvgEvidenceDistance_Std': df_s['AvgEvidenceDistance'].std(),
            'SampleCount': len(df_s),
        }
        
        agg_results.append(stats)
        
        print(f"\nç­–ç•¥ {strategy}:")
        print(f"  æ ·æœ¬æ•°: {stats['SampleCount']}")
        print(f"  å¹³å‡çª—å£äº‹ä»¶æ•°: {stats['WindowSize_Mean']:.0f}")
        print(f"  å¹³å‡Tokenæ•°: {stats['TokenCount_Mean']:.0f} Â± {stats['TokenCount_Std']:.0f}")
        print(f"  å¹³å‡ç½®ä¿¡åº¦: {stats['Confidence_Mean']:.4f} Â± {stats['Confidence_Std']:.4f}")
        print(f"  æ—©æœŸè¯æ®ç‡: {stats['EarlyEvidenceRate_Mean']:.2%} Â± {stats['EarlyEvidenceRate_Std']:.2%}")
        print(f"  å¹³å‡è¯æ®è·ç¦»: {stats['AvgEvidenceDistance_Mean']:.1f} Â± {stats['AvgEvidenceDistance_Std']:.1f} ä¸ªäº‹ä»¶")
    
    return pd.DataFrame(agg_results)


def plot_four_metrics(df_agg):
    """
    ç»˜åˆ¶4å¼ æŒ‡æ ‡å›¾
    
    æ¨ªåæ ‡ï¼šçª—å£äº‹ä»¶æ•°ã€ä¸Šä¸‹æ–‡tokenæ•°
    çºµåæ ‡ï¼šæ—©æœŸè¯æ®ç‡ã€å¹³å‡è¯æ®è·ç¦»
    """
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    fig.suptitle('ABCç­–ç•¥çš„è¯æ®è´¨é‡æŒ‡æ ‡å¯¹æ¯”', fontsize=16, fontweight='bold')
    
    strategies = ['A', 'B', 'C']
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']
    markers = ['o', 's', '^']
    
    # æå–æ•°æ®
    window_sizes = df_agg['WindowSize_Mean'].values
    token_counts = df_agg['TokenCount_Mean'].values
    token_stds = df_agg['TokenCount_Std'].values
    
    early_rates = df_agg['EarlyEvidenceRate_Mean'].values
    early_stds = df_agg['EarlyEvidenceRate_Std'].values
    
    distances = df_agg['AvgEvidenceDistance_Mean'].values
    distance_stds = df_agg['AvgEvidenceDistance_Std'].values
    
    # ==================== å›¾1: çª—å£äº‹ä»¶æ•° vs æ—©æœŸè¯æ®ç‡ ====================
    ax1 = axes[0, 0]
    for i, (strategy, color, marker) in enumerate(zip(strategies, colors, markers)):
        ax1.errorbar(window_sizes[i], early_rates[i], yerr=early_stds[i],
                    marker=marker, markersize=12, capsize=8, capthick=2,
                    linewidth=2, label=f'ç­–ç•¥{strategy}', color=color, alpha=0.8)
        # æ ‡æ³¨ç‚¹
        ax1.text(window_sizes[i], early_rates[i] + early_stds[i] + 0.03,
                f'{strategy}\n({early_rates[i]:.1%})',
                ha='center', va='bottom', fontsize=9, fontweight='bold')
    
    ax1.set_xlabel('çª—å£äº‹ä»¶æ•°', fontsize=12, fontweight='bold')
    ax1.set_ylabel('æ—©æœŸè¯æ®å¼•ç”¨ç‡', fontsize=12, fontweight='bold')
    ax1.set_title('çª—å£å¤§å° vs æ—©æœŸè¯æ®ç‡', fontsize=13, fontweight='bold')
    ax1.grid(True, alpha=0.3)
    ax1.legend(loc='best', fontsize=10)
    ax1.set_xscale('log')  # å¯¹æ•°åˆ»åº¦ï¼Œå› ä¸ºA=5, C=251å·®è·å¤§
    
    # ==================== å›¾2: çª—å£äº‹ä»¶æ•° vs å¹³å‡è¯æ®è·ç¦» ====================
    ax2 = axes[0, 1]
    for i, (strategy, color, marker) in enumerate(zip(strategies, colors, markers)):
        ax2.errorbar(window_sizes[i], distances[i], yerr=distance_stds[i],
                    marker=marker, markersize=12, capsize=8, capthick=2,
                    linewidth=2, label=f'ç­–ç•¥{strategy}', color=color, alpha=0.8)
        ax2.text(window_sizes[i], distances[i] + distance_stds[i] + 5,
                f'{strategy}\n({distances[i]:.0f})',
                ha='center', va='bottom', fontsize=9, fontweight='bold')
    
    ax2.set_xlabel('çª—å£äº‹ä»¶æ•°', fontsize=12, fontweight='bold')
    ax2.set_ylabel('å¹³å‡è¯æ®è·ç¦»ï¼ˆäº‹ä»¶æ•°ï¼‰', fontsize=12, fontweight='bold')
    ax2.set_title('çª—å£å¤§å° vs å¹³å‡è¯æ®è·ç¦»', fontsize=13, fontweight='bold')
    ax2.grid(True, alpha=0.3)
    ax2.legend(loc='best', fontsize=10)
    ax2.set_xscale('log')
    
    # ==================== å›¾3: ä¸Šä¸‹æ–‡Tokenæ•° vs æ—©æœŸè¯æ®ç‡ ====================
    ax3 = axes[1, 0]
    for i, (strategy, color, marker) in enumerate(zip(strategies, colors, markers)):
        ax3.errorbar(token_counts[i], early_rates[i], 
                    xerr=token_stds[i], yerr=early_stds[i],
                    marker=marker, markersize=12, capsize=8, capthick=2,
                    linewidth=2, label=f'ç­–ç•¥{strategy}', color=color, alpha=0.8)
        ax3.text(token_counts[i] + token_stds[i] + 200, early_rates[i],
                f'{strategy}',
                ha='left', va='center', fontsize=10, fontweight='bold')
    
    ax3.set_xlabel('ä¸Šä¸‹æ–‡Tokenæ•°', fontsize=12, fontweight='bold')
    ax3.set_ylabel('æ—©æœŸè¯æ®å¼•ç”¨ç‡', fontsize=12, fontweight='bold')
    ax3.set_title('Tokenæ¶ˆè€— vs æ—©æœŸè¯æ®ç‡', fontsize=13, fontweight='bold')
    ax3.grid(True, alpha=0.3)
    ax3.legend(loc='best', fontsize=10)
    
    # ==================== å›¾4: ä¸Šä¸‹æ–‡Tokenæ•° vs å¹³å‡è¯æ®è·ç¦» ====================
    ax4 = axes[1, 1]
    for i, (strategy, color, marker) in enumerate(zip(strategies, colors, markers)):
        ax4.errorbar(token_counts[i], distances[i],
                    xerr=token_stds[i], yerr=distance_stds[i],
                    marker=marker, markersize=12, capsize=8, capthick=2,
                    linewidth=2, label=f'ç­–ç•¥{strategy}', color=color, alpha=0.8)
        ax4.text(token_counts[i] + token_stds[i] + 200, distances[i],
                f'{strategy}',
                ha='left', va='center', fontsize=10, fontweight='bold')
    
    ax4.set_xlabel('ä¸Šä¸‹æ–‡Tokenæ•°', fontsize=12, fontweight='bold')
    ax4.set_ylabel('å¹³å‡è¯æ®è·ç¦»ï¼ˆäº‹ä»¶æ•°ï¼‰', fontsize=12, fontweight='bold')
    ax4.set_title('Tokenæ¶ˆè€— vs å¹³å‡è¯æ®è·ç¦»', fontsize=13, fontweight='bold')
    ax4.grid(True, alpha=0.3)
    ax4.legend(loc='best', fontsize=10)
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    plot_path = os.path.join(OUTPUT_DIR, "evidence_metrics_4plots.png")
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    print(f"\nâœ“ 4å¼ æŒ‡æ ‡å¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: {plot_path}")
    
    plt.close()


def plot_scatter_with_trend(df):
    """
    ç»˜åˆ¶æ•£ç‚¹å›¾ï¼ˆæ¯ä¸ªå¼‚å¸¸ç‚¹ä¸€ä¸ªç‚¹ï¼‰+ è¶‹åŠ¿çº¿
    
    å±•ç¤ºæ‰€æœ‰æ•°æ®ç‚¹çš„åˆ†å¸ƒï¼Œè€Œä¸ä»…ä»…æ˜¯å¹³å‡å€¼
    """
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    fig.suptitle('ABCç­–ç•¥è¯æ®æŒ‡æ ‡åˆ†å¸ƒï¼ˆæ‰€æœ‰å¼‚å¸¸ç‚¹ï¼‰', fontsize=16, fontweight='bold')
    
    strategies = ['A', 'B', 'C']
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']
    
    # ==================== å›¾1: çª—å£äº‹ä»¶æ•° vs æ—©æœŸè¯æ®ç‡ï¼ˆæ•£ç‚¹ï¼‰ ====================
    ax1 = axes[0, 0]
    for strategy, color in zip(strategies, colors):
        df_s = df[df['Strategy'] == strategy]
        # æ·»åŠ éšæœºæŠ–åŠ¨ï¼Œé¿å…ç‚¹é‡å 
        jitter = np.random.normal(0, 2, len(df_s))
        ax1.scatter(df_s['WindowSize'] + jitter, df_s['EarlyEvidenceRate'],
                   alpha=0.4, s=30, color=color, label=f'ç­–ç•¥{strategy}')
        
        # æ·»åŠ å¹³å‡å€¼æ ‡è®°
        mean_x = df_s['WindowSize'].mean()
        mean_y = df_s['EarlyEvidenceRate'].mean()
        ax1.scatter(mean_x, mean_y, marker='*', s=300, color=color,
                   edgecolors='black', linewidths=2, zorder=10)
    
    ax1.set_xlabel('çª—å£äº‹ä»¶æ•°', fontsize=11)
    ax1.set_ylabel('æ—©æœŸè¯æ®å¼•ç”¨ç‡', fontsize=11)
    ax1.set_title('çª—å£å¤§å° vs æ—©æœŸè¯æ®ç‡ï¼ˆæ•£ç‚¹åˆ†å¸ƒï¼‰', fontsize=12, fontweight='bold')
    ax1.set_xscale('log')
    ax1.legend(loc='best')
    ax1.grid(alpha=0.3)
    
    # ==================== å›¾2: çª—å£äº‹ä»¶æ•° vs å¹³å‡è·ç¦»ï¼ˆæ•£ç‚¹ï¼‰ ====================
    ax2 = axes[0, 1]
    for strategy, color in zip(strategies, colors):
        df_s = df[df['Strategy'] == strategy]
        jitter = np.random.normal(0, 2, len(df_s))
        ax2.scatter(df_s['WindowSize'] + jitter, df_s['AvgEvidenceDistance'],
                   alpha=0.4, s=30, color=color, label=f'ç­–ç•¥{strategy}')
        
        mean_x = df_s['WindowSize'].mean()
        mean_y = df_s['AvgEvidenceDistance'].mean()
        ax2.scatter(mean_x, mean_y, marker='*', s=300, color=color,
                   edgecolors='black', linewidths=2, zorder=10)
    
    ax2.set_xlabel('çª—å£äº‹ä»¶æ•°', fontsize=11)
    ax2.set_ylabel('å¹³å‡è¯æ®è·ç¦»ï¼ˆäº‹ä»¶æ•°ï¼‰', fontsize=11)
    ax2.set_title('çª—å£å¤§å° vs å¹³å‡è¯æ®è·ç¦»ï¼ˆæ•£ç‚¹åˆ†å¸ƒï¼‰', fontsize=12, fontweight='bold')
    ax2.set_xscale('log')
    ax2.legend(loc='best')
    ax2.grid(alpha=0.3)
    
    # ==================== å›¾3: Tokenæ•° vs æ—©æœŸè¯æ®ç‡ï¼ˆæ•£ç‚¹ï¼‰ ====================
    ax3 = axes[1, 0]
    for strategy, color in zip(strategies, colors):
        df_s = df[df['Strategy'] == strategy]
        ax3.scatter(df_s['TokenCount'], df_s['EarlyEvidenceRate'],
                   alpha=0.4, s=30, color=color, label=f'ç­–ç•¥{strategy}')
        
        mean_x = df_s['TokenCount'].mean()
        mean_y = df_s['EarlyEvidenceRate'].mean()
        ax3.scatter(mean_x, mean_y, marker='*', s=300, color=color,
                   edgecolors='black', linewidths=2, zorder=10)
    
    ax3.set_xlabel('ä¸Šä¸‹æ–‡Tokenæ•°', fontsize=11)
    ax3.set_ylabel('æ—©æœŸè¯æ®å¼•ç”¨ç‡', fontsize=11)
    ax3.set_title('Tokenæ¶ˆè€— vs æ—©æœŸè¯æ®ç‡ï¼ˆæ•£ç‚¹åˆ†å¸ƒï¼‰', fontsize=12, fontweight='bold')
    ax3.legend(loc='best')
    ax3.grid(alpha=0.3)
    
    # ==================== å›¾4: Tokenæ•° vs å¹³å‡è·ç¦»ï¼ˆæ•£ç‚¹ï¼‰ ====================
    ax4 = axes[1, 1]
    for strategy, color in zip(strategies, colors):
        df_s = df[df['Strategy'] == strategy]
        ax4.scatter(df_s['TokenCount'], df_s['AvgEvidenceDistance'],
                   alpha=0.4, s=30, color=color, label=f'ç­–ç•¥{strategy}')
        
        mean_x = df_s['TokenCount'].mean()
        mean_y = df_s['AvgEvidenceDistance'].mean()
        ax4.scatter(mean_x, mean_y, marker='*', s=300, color=color,
                   edgecolors='black', linewidths=2, zorder=10)
    
    ax4.set_xlabel('ä¸Šä¸‹æ–‡Tokenæ•°', fontsize=11)
    ax4.set_ylabel('å¹³å‡è¯æ®è·ç¦»ï¼ˆäº‹ä»¶æ•°ï¼‰', fontsize=11)
    ax4.set_title('Tokenæ¶ˆè€— vs å¹³å‡è¯æ®è·ç¦»ï¼ˆæ•£ç‚¹åˆ†å¸ƒï¼‰', fontsize=12, fontweight='bold')
    ax4.legend(loc='best')
    ax4.grid(alpha=0.3)
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    plot_path = os.path.join(OUTPUT_DIR, "evidence_metrics_scatter.png")
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    print(f"âœ“ æ•£ç‚¹åˆ†å¸ƒå›¾å·²ä¿å­˜åˆ°: {plot_path}")
    
    plt.close()


def plot_combined_4metrics(df_agg):
    """
    ç»˜åˆ¶4å¼ ç‹¬ç«‹æŒ‡æ ‡å›¾ï¼ˆæ¯å¼ å›¾åªå…³æ³¨ä¸€ä¸ªæŒ‡æ ‡ï¼‰
    ä½¿ç”¨æŠ˜çº¿å›¾ + è¯¯å·®å¸¦ï¼Œæ›´æ¸…æ™°åœ°å±•ç¤ºè¶‹åŠ¿
    """
    fig, axes = plt.subplots(2, 2, figsize=(15, 11))
    fig.suptitle('ABCç­–ç•¥çš„è¯æ®è´¨é‡æŒ‡æ ‡å®Œæ•´å¯¹æ¯”', fontsize=16, fontweight='bold')
    
    strategies = df_agg['Strategy'].values
    x_positions = [0, 1, 2]  # A, B, Cçš„ä½ç½®
    
    # æå–æ•°æ®
    window_sizes = df_agg['WindowSize_Mean'].values
    token_counts = df_agg['TokenCount_Mean'].values
    token_stds = df_agg['TokenCount_Std'].values
    
    early_rates = df_agg['EarlyEvidenceRate_Mean'].values
    early_stds = df_agg['EarlyEvidenceRate_Std'].values
    
    distances = df_agg['AvgEvidenceDistance_Mean'].values
    distance_stds = df_agg['AvgEvidenceDistance_Std'].values
    
    # ==================== å›¾1: çª—å£äº‹ä»¶æ•° vs æ—©æœŸè¯æ®ç‡ ====================
    ax1 = axes[0, 0]
    ax1.errorbar(window_sizes, early_rates, yerr=early_stds,
                marker='o', markersize=10, capsize=8, capthick=2,
                linewidth=2.5, color='#45B7D1', alpha=0.8)
    
    for i, (ws, er, strategy) in enumerate(zip(window_sizes, early_rates, strategies)):
        ax1.text(ws, er + early_stds[i] + 0.03,
                f'{strategy}: {er:.1%}',
                ha='center', va='bottom', fontsize=10, fontweight='bold',
                bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7))
    
    ax1.set_xlabel('çª—å£äº‹ä»¶æ•°', fontsize=12, fontweight='bold')
    ax1.set_ylabel('æ—©æœŸè¯æ®å¼•ç”¨ç‡', fontsize=12, fontweight='bold')
    ax1.set_title('(1) çª—å£å¤§å° â†’ æ—©æœŸè¯æ®ç‡', fontsize=13, fontweight='bold')
    ax1.grid(True, alpha=0.3, linestyle='--')
    ax1.set_xscale('log')
    
    # ==================== å›¾2: çª—å£äº‹ä»¶æ•° vs å¹³å‡è¯æ®è·ç¦» ====================
    ax2 = axes[0, 1]
    ax2.errorbar(window_sizes, distances, yerr=distance_stds,
                marker='s', markersize=10, capsize=8, capthick=2,
                linewidth=2.5, color='#FF6B6B', alpha=0.8)
    
    for i, (ws, dist, strategy) in enumerate(zip(window_sizes, distances, strategies)):
        ax2.text(ws, dist + distance_stds[i] + 5,
                f'{strategy}: {dist:.0f}',
                ha='center', va='bottom', fontsize=10, fontweight='bold',
                bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7))
    
    ax2.set_xlabel('çª—å£äº‹ä»¶æ•°', fontsize=12, fontweight='bold')
    ax2.set_ylabel('å¹³å‡è¯æ®è·ç¦»ï¼ˆäº‹ä»¶æ•°ï¼‰', fontsize=12, fontweight='bold')
    ax2.set_title('(2) çª—å£å¤§å° â†’ å¹³å‡è¯æ®è·ç¦»', fontsize=13, fontweight='bold')
    ax2.grid(True, alpha=0.3, linestyle='--')
    ax2.set_xscale('log')
    
    # ==================== å›¾3: ä¸Šä¸‹æ–‡Tokenæ•° vs æ—©æœŸè¯æ®ç‡ ====================
    ax3 = axes[1, 0]
    ax3.errorbar(token_counts, early_rates, xerr=token_stds, yerr=early_stds,
                marker='^', markersize=10, capsize=8, capthick=2,
                linewidth=2.5, color='#4ECDC4', alpha=0.8)
    
    for i, (tc, er, strategy) in enumerate(zip(token_counts, early_rates, strategies)):
        ax3.text(tc, er + early_stds[i] + 0.03,
                f'{strategy}\n{tc:.0f} tokens',
                ha='center', va='bottom', fontsize=9, fontweight='bold',
                bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7))
    
    ax3.set_xlabel('ä¸Šä¸‹æ–‡Tokenæ•°', fontsize=12, fontweight='bold')
    ax3.set_ylabel('æ—©æœŸè¯æ®å¼•ç”¨ç‡', fontsize=12, fontweight='bold')
    ax3.set_title('(3) Tokenæ¶ˆè€— â†’ æ—©æœŸè¯æ®ç‡', fontsize=13, fontweight='bold')
    ax3.grid(True, alpha=0.3, linestyle='--')
    
    # ==================== å›¾4: ä¸Šä¸‹æ–‡Tokenæ•° vs å¹³å‡è¯æ®è·ç¦» ====================
    ax4 = axes[1, 1]
    ax4.errorbar(token_counts, distances, xerr=token_stds, yerr=distance_stds,
                marker='D', markersize=10, capsize=8, capthick=2,
                linewidth=2.5, color='#FFA07A', alpha=0.8)
    
    for i, (tc, dist, strategy) in enumerate(zip(token_counts, distances, strategies)):
        ax4.text(tc, dist + distance_stds[i] + 5,
                f'{strategy}\n{tc:.0f} tokens',
                ha='center', va='bottom', fontsize=9, fontweight='bold',
                bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7))
    
    ax4.set_xlabel('ä¸Šä¸‹æ–‡Tokenæ•°', fontsize=12, fontweight='bold')
    ax4.set_ylabel('å¹³å‡è¯æ®è·ç¦»ï¼ˆäº‹ä»¶æ•°ï¼‰', fontsize=12, fontweight='bold')
    ax4.set_title('(4) Tokenæ¶ˆè€— â†’ å¹³å‡è¯æ®è·ç¦»', fontsize=13, fontweight='bold')
    ax4.grid(True, alpha=0.3, linestyle='--')
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    plot_path = os.path.join(OUTPUT_DIR, "evidence_metrics_combined.png")
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    print(f"âœ“ ç»„åˆæŒ‡æ ‡å›¾å·²ä¿å­˜åˆ°: {plot_path}")
    
    plt.close()


def save_detailed_report(df, df_agg):
    """ä¿å­˜è¯¦ç»†åˆ†ææŠ¥å‘Š"""
    output_path = os.path.join(OUTPUT_DIR, "evidence_metrics_report.xlsx")
    
    with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
        # Sheet 1: èšåˆç»Ÿè®¡
        df_agg.to_excel(writer, sheet_name='èšåˆç»Ÿè®¡', index=False)
        
        # Sheet 2: åŸå§‹æ•°æ®
        df.to_excel(writer, sheet_name='åŸå§‹æ•°æ®', index=False)
        
        # Sheet 3: æŒ‰å‚ä¸è€…èšåˆ
        df_by_p = df.groupby(['Participant', 'Strategy']).agg({
            'WindowSize': 'mean',
            'TokenCount': 'mean',
            'EarlyEvidenceRate': 'mean',
            'AvgEvidenceDistance': 'mean',
            'Confidence': 'mean',
        }).reset_index()
        df_by_p.to_excel(writer, sheet_name='æŒ‰å‚ä¸è€…èšåˆ', index=False)
    
    print(f"âœ“ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")


def interpret_results(df_agg):
    """è§£è¯»å®éªŒç»“æœ"""
    print("\n" + "="*80)
    print("ğŸ” å®éªŒç»“æœè§£è¯»")
    print("="*80)
    
    # æå–å…³é”®æ•°æ®
    a_data = df_agg[df_agg['Strategy'] == 'A'].iloc[0]
    b_data = df_agg[df_agg['Strategy'] == 'B'].iloc[0]
    c_data = df_agg[df_agg['Strategy'] == 'C'].iloc[0]
    
    print("\n1ï¸âƒ£ æ—©æœŸè¯æ®å¼•ç”¨ç‡åˆ†æ:")
    print(f"   ç­–ç•¥A: {a_data['EarlyEvidenceRate_Mean']:.2%}")
    print(f"   ç­–ç•¥B: {b_data['EarlyEvidenceRate_Mean']:.2%}")
    print(f"   ç­–ç•¥C: {c_data['EarlyEvidenceRate_Mean']:.2%}")
    
    if c_data['EarlyEvidenceRate_Mean'] > a_data['EarlyEvidenceRate_Mean']:
        improvement = (c_data['EarlyEvidenceRate_Mean'] - a_data['EarlyEvidenceRate_Mean']) * 100
        print(f"\n   âœ… Cç­–ç•¥æ¯”Aç­–ç•¥å¤šå¼•ç”¨äº† {improvement:.1f}% çš„æ—©æœŸè¯æ®")
        print(f"   â†’ è¯´æ˜ï¼šé•¿çª—å£è®©LLMèƒ½å¤Ÿè®¿é—®æ›´æ—©çš„å†å²ä¿¡æ¯")
        print(f"   â†’ éªŒè¯äº†ï¼šç”¨æˆ·æ„å›¾çš„çº¿ç´¢ç¡®å®åˆ†å¸ƒåœ¨è¾ƒé•¿æ—¶é—´è·¨åº¦å†…")
    else:
        print(f"\n   âš ï¸ Cç­–ç•¥çš„æ—©æœŸè¯æ®ç‡å¹¶æœªæ˜¾è‘—æå‡")
        print(f"   â†’ å¯èƒ½åŸå› ï¼šLLMä»ç„¶å€¾å‘äºä¾èµ–è¿‘æœŸä¿¡æ¯")
    
    print("\n2ï¸âƒ£ å¹³å‡è¯æ®è·ç¦»åˆ†æ:")
    print(f"   ç­–ç•¥A: {a_data['AvgEvidenceDistance_Mean']:.1f} ä¸ªäº‹ä»¶")
    print(f"   ç­–ç•¥B: {b_data['AvgEvidenceDistance_Mean']:.1f} ä¸ªäº‹ä»¶")
    print(f"   ç­–ç•¥C: {c_data['AvgEvidenceDistance_Mean']:.1f} ä¸ªäº‹ä»¶")
    
    if c_data['AvgEvidenceDistance_Mean'] > a_data['AvgEvidenceDistance_Mean']:
        print(f"\n   âœ… Cç­–ç•¥çš„å¹³å‡è¯æ®è·ç¦»æ›´å¤§")
        print(f"   â†’ è¯´æ˜ï¼šé•¿çª—å£è®©LLMèƒ½å¤Ÿå¼•ç”¨æ›´è¿œçš„å†å²äº‹ä»¶ä½œä¸ºè¯æ®")
        print(f"   â†’ éªŒè¯äº†ï¼šé•¿æœŸè®°å¿†æœºåˆ¶èƒ½å¤Ÿæ•è·è¿œæœŸçº¿ç´¢")
    else:
        print(f"\n   âš ï¸ å³ä½¿çª—å£å˜é•¿ï¼Œè¯æ®è·ç¦»æœªæ˜¾è‘—å¢åŠ ")
        print(f"   â†’ å¯èƒ½åŸå› ï¼šLLMçš„æ³¨æ„åŠ›ä»é›†ä¸­åœ¨è¿‘æœŸï¼ˆrecency biasï¼‰")
    
    print("\n3ï¸âƒ£ Tokenæ•ˆç‡åˆ†æ:")
    print(f"   ç­–ç•¥A: {a_data['TokenCount_Mean']:.0f} tokens â†’ ç½®ä¿¡åº¦ {a_data['Confidence_Mean']:.3f}")
    print(f"   ç­–ç•¥B: {b_data['TokenCount_Mean']:.0f} tokens â†’ ç½®ä¿¡åº¦ {b_data['Confidence_Mean']:.3f}")
    print(f"   ç­–ç•¥C: {c_data['TokenCount_Mean']:.0f} tokens â†’ ç½®ä¿¡åº¦ {c_data['Confidence_Mean']:.3f}")
    
    # è®¡ç®—tokenæ•ˆç‡ï¼ˆç½®ä¿¡åº¦æå‡ / tokenå¢åŠ ï¼‰
    token_increase_b = b_data['TokenCount_Mean'] - a_data['TokenCount_Mean']
    conf_increase_b = b_data['Confidence_Mean'] - a_data['Confidence_Mean']
    efficiency_b = conf_increase_b / token_increase_b if token_increase_b > 0 else 0
    
    token_increase_c = c_data['TokenCount_Mean'] - a_data['TokenCount_Mean']
    conf_increase_c = c_data['Confidence_Mean'] - a_data['Confidence_Mean']
    efficiency_c = conf_increase_c / token_increase_c if token_increase_c > 0 else 0
    
    print(f"\n   Tokenæ•ˆç‡ (ç½®ä¿¡åº¦æå‡ / Tokenå¢åŠ ):")
    print(f"   ç­–ç•¥B: {efficiency_b*10000:.2f} ç½®ä¿¡åº¦æå‡ / åƒtoken")
    print(f"   ç­–ç•¥C: {efficiency_c*10000:.2f} ç½®ä¿¡åº¦æå‡ / åƒtoken")
    
    if efficiency_b > efficiency_c:
        print(f"\n   âš ï¸ ç­–ç•¥Bçš„Tokenæ•ˆç‡æ›´é«˜")
        print(f"   â†’ å»ºè®®ï¼šå¦‚æœè®¡ç®—èµ„æºæœ‰é™ï¼Œç­–ç•¥Bå¯èƒ½æ˜¯æ›´å¥½çš„å¹³è¡¡ç‚¹")
    else:
        print(f"\n   âœ… ç­–ç•¥Cè™½ç„¶æ¶ˆè€—æ›´å¤šTokenï¼Œä½†æ•ˆç‡ä»ç„¶æ›´é«˜")
        print(f"   â†’ å»ºè®®ï¼šå¦‚æœè¿½æ±‚æœ€é«˜å‡†ç¡®ç‡ï¼Œåº”ä½¿ç”¨ç­–ç•¥C")


def main():
    print("="*80)
    print("ğŸ“Š è¯æ®è´¨é‡æŒ‡æ ‡åˆ†æ - ABCç­–ç•¥å¯¹æ¯”")
    print("="*80)
    
    # åŠ è½½å¹¶å¤„ç†æ•°æ®
    df = load_and_process_data()
    if df is None:
        return
    
    # èšåˆç»Ÿè®¡
    df_agg = aggregate_by_strategy(df)
    
    # ç»˜åˆ¶4å¼ æŒ‡æ ‡å›¾ï¼ˆç»„åˆç‰ˆï¼Œæ¸…æ™°å±•ç¤ºè¶‹åŠ¿ï¼‰
    print("\n" + "="*80)
    print("ğŸ“ˆ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    print("="*80)
    
    plot_combined_4metrics(df_agg)
    
    # ç»˜åˆ¶æ•£ç‚¹åˆ†å¸ƒå›¾ï¼ˆå±•ç¤ºæ‰€æœ‰æ•°æ®ç‚¹ï¼‰
    plot_scatter_with_trend(df)
    
    # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    save_detailed_report(df, df_agg)
    
    # è§£è¯»ç»“æœ
    interpret_results(df_agg)
    
    print("\n" + "="*80)
    print("âœ… è¯æ®è´¨é‡æŒ‡æ ‡åˆ†æå®Œæˆï¼")
    print("="*80)
    print("\nç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"  1. {os.path.join(OUTPUT_DIR, 'evidence_metrics_report.xlsx')}")
    print(f"  2. {os.path.join(OUTPUT_DIR, 'evidence_metrics_combined.png')} â­ ä¸»å›¾")
    print(f"  3. {os.path.join(OUTPUT_DIR, 'evidence_metrics_scatter.png')}")
    print("\nğŸ“– è¯´æ˜:")
    print("  - evidence_metrics_combined.png: 4å¼ æ¸…æ™°çš„è¶‹åŠ¿å›¾ï¼ˆæ¨èç”¨äºè®ºæ–‡ï¼‰")
    print("  - evidence_metrics_scatter.png: æ•£ç‚¹åˆ†å¸ƒå›¾ï¼ˆå±•ç¤ºæ•°æ®åˆ†å¸ƒï¼‰")
    print("  - evidence_metrics_report.xlsx: è¯¦ç»†æ•°æ®ï¼ˆ3ä¸ªSheetï¼‰")


if __name__ == "__main__":
    main()
