"""
è®¡ç®—Tokenä½¿ç”¨é‡

ç”¨é€”ï¼šä¼°ç®—å•ä¸ªå‚ä¸Žè€…å®Œæ•´æµç¨‹çš„Tokenæ¶ˆè€—
"""

import sys
import os
sys.path.insert(0, os.path.dirname(__file__))

from data_loader import DataLoader
from anomaly_detector import AnomalyDetector
from config import *
from context_builder import normalize_behavior_sequence
from key_event_selector import select_key_events
from memory_bank import MemoryBank, chunk_events, summarize_chunk
from window_and_compress import build_window, compress_events, format_events_for_prompt
from intent_prompting import build_intent_prompt
from context_builder import find_nearest_event_idx, find_nearest_key_event_pos


def estimate_tokens(text: str) -> int:
    """
    ä¼°ç®—Tokenæ•°é‡
    
    ç»éªŒè§„åˆ™ï¼š
    - è‹±æ–‡: ~4 chars/token
    - ä¸­æ–‡: ~2 chars/token
    - æ•°å­—/ç¬¦å·: ~1 char/token
    
    ç®€åŒ–ä¼°ç®—ï¼šå–å¹³å‡å€¼ ~3 chars/token
    """
    return len(text) // 3


def analyze_participant_tokens(p_id: str = "P1"):
    """åˆ†æžå•ä¸ªå‚ä¸Žè€…çš„Tokenä½¿ç”¨æƒ…å†µ"""
    
    print("=" * 80)
    print(f"ðŸ” Tokenä½¿ç”¨é‡åˆ†æž: {p_id}")
    print("=" * 80)
    
    loader = DataLoader(DATA_DIR)
    detector = AnomalyDetector(config={})
    
    # 1. åŠ è½½æ•°æ®
    raw_seq = loader.load_behavior_sequence(p_id)
    events = normalize_behavior_sequence(raw_seq)
    
    print(f"\nðŸ“Š åŽŸå§‹æ•°æ®:")
    print(f"  åŽŸå§‹äº‹ä»¶æ•°: {len(events)}")
    
    # ä¼°ç®—åŽŸå§‹äº‹ä»¶æ ¼å¼åŒ–åŽçš„Tokenæ•°
    if events:
        sample_event = events[0]
        sample_text = f"- idx={sample_event.idx} t={sample_event.t}->{sample_event.t} page={sample_event.page} widget={sample_event.widget} op={sample_event.op} count=1"
        tokens_per_event = estimate_tokens(sample_text)
        total_raw_tokens = tokens_per_event * len(events)
        print(f"  å•ä¸ªäº‹ä»¶Tokenæ•°: ~{tokens_per_event}")
        print(f"  å…¨éƒ¨åŽŸå§‹äº‹ä»¶Tokenæ•°: ~{total_raw_tokens:,} tokens ({total_raw_tokens/1000:.1f}k)")
    
    # 2. æ£€æµ‹å¼‚å¸¸
    anomalies = detector.detect_anomalies(raw_seq)
    print(f"\nðŸ” å¼‚å¸¸ç‚¹:")
    print(f"  æ£€æµ‹åˆ°å¼‚å¸¸æ•°: {len(anomalies)}")
    
    # 3. é€‰æ‹©å…³é”®äº‹ä»¶
    key_events = select_key_events(
        events,
        target_k=KEY_EVENT_TARGET_K,
        num_bins=KEY_EVENT_NUM_BINS,
        top_m_per_bin=KEY_EVENT_TOP_M_PER_BIN,
        near_dt_ms=KEY_EVENT_NEAR_DT_MS,
    )
    print(f"\nâ­ å…³é”®äº‹ä»¶:")
    print(f"  é€‰æ‹©çš„å…³é”®äº‹ä»¶æ•°: {len(key_events)}")
    
    # 4. æž„å»ºLTM
    mb = MemoryBank(max_items=MEMORY_MAX_ITEMS)
    chunks = chunk_events(key_events, MEMORY_CHUNK_SIZE)
    print(f"\nðŸ§  é•¿æœŸè®°å¿†:")
    print(f"  LTM chunksæ•°: {len(chunks)}")
    
    ltm_total_tokens = 0
    for ci, ch in enumerate(chunks):
        item = summarize_chunk(ch, chunk_id=f"{p_id}_{ci}")
        mb.add(item)
        ltm_total_tokens += estimate_tokens(item.summary)
    
    print(f"  å…¨éƒ¨LTMæ‘˜è¦Tokenæ•°: ~{ltm_total_tokens:,} tokens ({ltm_total_tokens/1000:.1f}k)")
    
    # 5. æ¨¡æ‹Ÿå•æ¬¡æŽ¨ç†çš„Tokenä½¿ç”¨
    if anomalies:
        anomaly = anomalies[0]
        timestamp = int(anomaly.get("timestamp", 0))
        task_info = TASK_DEFINITIONS["Task1"]
        
        center_pos = find_nearest_event_idx(events, timestamp)
        if center_pos is not None:
            center_event = events[center_pos]
            key_center_pos = find_nearest_key_event_pos(key_events, center_event)
            
            if key_center_pos is not None:
                print(f"\nðŸ“ å•æ¬¡æŽ¨ç†Tokenåˆ†æž (ç¬¬1ä¸ªå¼‚å¸¸ç‚¹):")
                print(f"  å¼‚å¸¸æ—¶é—´: {timestamp}ms")
                
                # æ£€ç´¢LTM
                query_pages = {center_event.page} if center_event.page != "None" else set()
                query_widgets = {center_event.widget} if center_event.widget != "None" else set()
                query_ops = {center_event.op} if center_event.op != "None" else set()
                ltm_items = mb.retrieve(query_pages, query_widgets, query_ops, top_k=MEMORY_RETRIEVE_TOP_K)
                
                ltm_text_tokens = sum(estimate_tokens(it.summary) for it in ltm_items)
                print(f"  æ£€ç´¢åˆ°çš„LTM Tokenæ•°: ~{ltm_text_tokens:,} tokens")
                
                # æµ‹è¯•3ç§ç­–ç•¥
                for strategy in ["A", "B", "C"]:
                    win = build_window(
                        key_events=key_events,
                        center_pos=key_center_pos,
                        mode=strategy,
                        window_mode=WINDOW_MODE,
                        strategy_windows=STRATEGY_WINDOWS,
                    )
                    compressed = compress_events(win, merge_consecutive=COMPRESS_MERGE_CONSECUTIVE)
                    stm_text = format_events_for_prompt(compressed, max_lines=PROMPT_MAX_EVENT_LINES)
                    
                    prompt = build_intent_prompt(
                        task_info=task_info,
                        anomaly=anomaly,
                        strategy=strategy,
                        stm_events_text=stm_text,
                        ltm_items=ltm_items,
                        intent_labels=INTENT_LABELS,
                    )
                    
                    prompt_tokens = estimate_tokens(prompt)
                    stm_tokens = estimate_tokens(stm_text)
                    
                    print(f"\n  ç­–ç•¥{strategy}:")
                    print(f"    STMçª—å£äº‹ä»¶æ•°: {len(win)}")
                    print(f"    STM Tokenæ•°: ~{stm_tokens:,} tokens")
                    print(f"    å®Œæ•´Prompt Tokenæ•°: ~{prompt_tokens:,} tokens ({prompt_tokens/1000:.1f}k)")
    
    # 6. ä¼°ç®—å…¨æµç¨‹Tokenä½¿ç”¨
    print(f"\n" + "=" * 80)
    print(f"ðŸ“Š å…¨æµç¨‹Tokenä¼°ç®—:")
    print("=" * 80)
    
    if anomalies and center_pos is not None and key_center_pos is not None:
        # ä½¿ç”¨ç­–ç•¥Cçš„Tokenæ•°ä½œä¸ºä»£è¡¨
        win_c = build_window(
            key_events=key_events,
            center_pos=key_center_pos,
            mode="C",
            window_mode=WINDOW_MODE,
            strategy_windows=STRATEGY_WINDOWS,
        )
        compressed_c = compress_events(win_c, merge_consecutive=COMPRESS_MERGE_CONSECUTIVE)
        stm_text_c = format_events_for_prompt(compressed_c, max_lines=PROMPT_MAX_EVENT_LINES)
        prompt_c = build_intent_prompt(
            task_info=task_info,
            anomaly=anomaly,
            strategy="C",
            stm_events_text=stm_text_c,
            ltm_items=ltm_items,
            intent_labels=INTENT_LABELS,
        )
        
        avg_tokens_per_inference = estimate_tokens(prompt_c)
        num_inferences = len(anomalies) * 3  # æ¯ä¸ªå¼‚å¸¸ç‚¹3ä¸ªç­–ç•¥
        total_tokens = avg_tokens_per_inference * num_inferences
        
        print(f"\næ¯æ¬¡æŽ¨ç†å¹³å‡Tokenæ•°ï¼ˆç­–ç•¥Cï¼‰: ~{avg_tokens_per_inference:,} tokens")
        print(f"å¼‚å¸¸ç‚¹æ•°: {len(anomalies)}")
        print(f"æŽ¨ç†æ¬¡æ•°ï¼ˆæ¯ä¸ªå¼‚å¸¸Ã—3ç­–ç•¥ï¼‰: {num_inferences}")
        print(f"\nâœ¨ æ€»Tokenæ¶ˆè€—ä¼°ç®—: ~{total_tokens:,} tokens ({total_tokens/1000:.1f}k)")
        
        if total_tokens > 200000:
            print(f"\nâš ï¸  æ³¨æ„ï¼šTokenæ•°è¶…è¿‡20ä¸‡ï¼å»ºè®®:")
            print(f"  1. å‡å°‘å¼‚å¸¸ç‚¹æ•°é‡")
            print(f"  2. å‡å°‘KEY_EVENT_TARGET_K")
            print(f"  3. å‡å°STRATEGY_WINDOWS['C']çš„çª—å£å¤§å°")
    
    print("\n" + "=" * 80)


if __name__ == "__main__":
    # å¯ä»¥æŒ‡å®šå‚ä¸Žè€…ID
    import sys
    p_id = sys.argv[1] if len(sys.argv) > 1 else "P1"
    analyze_participant_tokens(p_id)
