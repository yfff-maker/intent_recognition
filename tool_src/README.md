# Approach Reproduction Source Code

This folder contains the source code to reproduce the **LLM-based Evolutionary Requirements Elicitation** method described in the paper. It provides an end-to-end pipeline from the anonymized dataset (`anonymous_data`) to inferred requirements.

## Project Structure

```
tool_src/
├── config.py             # Configuration settings (paths, API keys, thresholds)
├── data_loader.py        # Handles loading of JSON sequences and video paths
├── anomaly_detector.py   # Implements rule-based anomaly detection (e.g., repetitive clicks)
├── context_builder.py        # (Original reproduction) Extracts video frames and constructs LLM prompts
├── llm_client.py             # LLM interface (default mocked; can be wired to real API)
├── event_representation.py   # Step0: unify raw logs into Event list with stable idx
├── key_event_selector.py     # Step1: select key events (importance + coverage) for token control
├── window_and_compress.py    # Step2/3: build A/B/C windows and compress into stable evidence text
├── memory_bank.py            # Step4-6: chunk→summary→store→retrieve (lightweight LTM)
├── intent_prompting.py       # Step7: build intent prompt + parse JSON output
├── main.py                   # Main script (supports INTENT mode without MP4)
└── output/               # Generated results and cached frames
```

## Prerequisites

1.  **Python 3.8+**
2.  **Dependencies**:
    ```bash
    pip install pandas opencv-python openpyxl
    ```
3.  **Dataset**: Ensure the `anonymous_data` folder is located at `../anonymous_data` relative to this folder.

## How to Run

1.  **Configure**: Edit `config.py` to set your preferences.
    - **API Mode**: Set `LLM_INTERACTION_MODE = "API"` and provide `OPENROUTER_API_KEY`.
    - **Web UI Mode**: Set `LLM_INTERACTION_MODE = "WEB_UI"` (Default). This generates a guide for manual interaction with ChatGPT.
    - **Task Switch**:
        - `LLM_TASK = "REQUIREMENTS"`: original paper-style requirements elicitation (may require MP4 if you use `context_builder.py`).
        - `LLM_TASK = "INTENT"`: long-sequence intent inference with controllable context length (no MP4 required).
    - **Provider**: This project uses OpenRouter only.
      - Recommended: set `OPENROUTER_SITE_URL` and `OPENROUTER_APP_NAME` for attribution headers.

2.  **Execute**:
    ```bash
    python main.py
    ```

3.  **Output**:
    - **API Mode**: Results are saved to `output/inferred_requirements.xlsx`.
    - **Web UI Mode**: A guide is generated at `output/manual_interaction_guide.md`. Open this file to see the extracted frames and pre-generated prompts. You can copy-paste them into the ChatGPT web interface.
    - **INTENT Mode**: Results are saved to `output/intent_inference_results.xlsx` (one anchor run with A/B/C strategies).

## Implementation Details

### 1. Anomaly Detection (`anomaly_detector.py`)
We implement a simplified version of the rule-based detection described in Section III.C of the paper.
- **Repetitive Interactions**: Detects if the same widget is interacted with >= 3 times consecutively.
- **Long Duration**: Detects if a user stays on a page/action for > 5 seconds without progress.

### 2. Context Extraction (`context_builder.py`)
- Uses `opencv` to seek to the exact timestamp of the anomaly in the video recording.
- Saves the frame as a JPEG image.
- Constructs a structured prompt including:
    - **Goal Context**: Defined in `config.py`.
    - **GUI Info**: Reference to the extracted frame.
    - **Anomaly Description**: Generated by the detector.

### 3. Long-Sequence Intent Inference (Extension)
This repository additionally supports a controlled long-sequence experiment that makes *"how much behavior history the LLM needs"* an explicit ablation variable.
For the same anchor (anomaly timestamp), the pipeline runs three strategies:
- **A (Short)**: minimal window (near-local evidence)
- **B (Meso)**: mid window (pattern-level evidence)
- **C (Long)**: long window (global-in-sequence evidence, still token-controlled by key-event selection/compression)

Implementation mapping (Step0–Step7):
- **Step0 Unified Representation**: `event_representation.py`
- **Step1 Key Event Selection**: `key_event_selector.py`
- **Step2 Window Builder (A/B/C)**: `window_and_compress.py`
- **Step3 Prompt Compression**: `window_and_compress.py`
- **Step4-6 LTM (chunk→summary→retrieve)**: `memory_bank.py`
- **Step7 Prompt + JSON Output Parsing**: `intent_prompting.py`

### 4. LLM Inference (`llm_client.py`)
- Simulates the role of a "Requirement Analyst".
- Uses the prompt structure defined in Table III of the paper.
- **Note**: The default code uses a mock response to avoid consuming API credits. Uncomment the API call lines in `llm_client.py` to use real GPT-3.5/4.

## Extending the Pipeline

To fully replicate the paper's results:
1.  **Refine Task Context**: Update `TASK_DEFINITIONS` in `config.py` with the specific tasks used in your experiment.
2.  **Enhance GUI Analysis**: Integrate an Image-to-Text model (like GPT-4 Vision or a local OCR tool) in `context_builder.py` to generate textual descriptions of the interface from the extracted frames.
3.  **Prioritization**: Implement the UX Score calculation (using `raw_data` metrics) to sort the rows in the final Excel output.
